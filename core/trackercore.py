##
# Tracker Core
#
# This file is part of the Robotic Teleoperation via Motion and Gaze Tracking
# MQP from the Worcester Polytechinc Institute.
#
# Copyright 2021 Nicholas Hollander <nhhollander@wpi.edu>
#

import os
import pathlib
import dlib
import pyrealsense2 as rs
import numpy as np
import cv2

import debug
import util

# Tracker data
data = {
    "status": "NOT_LOADED"
}
# Configuration Options
config = {
    "face_detection_scale_factor": 0.25,
    "enable_debug_draw": True
}

##-----------##
## Main Loop ##
##-----------##

@debug.funcperf
def loop_once():
    '''
    Runs a single iteration of the processing loop.
    '''
    if data['status'] != "READY":
        util.log(f"Not Looping - State is not READY ({data['status']})", level=util.Levels.ERROR)
        return
    try:
        acquire_image()
        preprocess_images()
        find_face()
        debug_draw_face('color_frame')
    except util.Abort as e:
        util.log(f"Processing aborted: {e.message}", e.level)

##----------------------##
## Main Process Methods ##
##----------------------##

@debug.funcperf
def init():
    '''
    Initialize the tracker core.
    '''
    util.log("\033[1mInitializing Tracker Core...\033[0m")
    data['status'] = "INITIALIZING"

    util.log("Initializing RealSense camera")
    data['rs_pipeline'] = rs.pipeline()
    data['rs_config'] = rs.config()
    data['rs_config'].enable_stream(rs.stream.color, width=1920, height=1080, format=rs.format.rgb8)
    data['rs_config'].enable_stream(rs.stream.depth, width=1280, height=720, format=rs.format.z16)
    data['rs_profile'] = data['rs_pipeline'].start(data['rs_config'])
    data['rs_device'] = data['rs_profile'].get_device()
    data['rs_depthsensor'] = data['rs_device'].query_sensors()[0]

    util.log("Initializing Face Detector")
    data['face_detector'] = dlib.get_frontal_face_detector()
    predictor_path = str(pathlib.Path(__file__).parent.absolute()) + \
        "/shape_predictor_68_face_landmarks.dat"
    if not os.path.exists(predictor_path):
        util.log("Unable to locate face prediction model - see README.md", level=util.Levels.ERROR)
        data['status'] = "ERROR"
        return
    data['face_predictor'] = dlib.shape_predictor(predictor_path)

    util.log("Initialization Complete!")
    data['status'] = "READY"


@debug.funcperf
def acquire_image():
    '''
    Attempts to acquire a new set of images from the RealSense camera.
    '''
    if data['status'] != "READY":
        raise util.Abort("Unable to acquire image - system not ready", util.Levels.ERROR)
    # Wait until both frames are ready
    while True:
        frames = data['rs_pipeline'].wait_for_frames()
        depth = frames.get_depth_frame()
        color = frames.get_color_frame()
        if depth is not None:
            break
    data['color_raw'] = color
    data['depth_raw'] = depth

@debug.funcperf
def preprocess_images():
    '''
    The RGB and Depth frames generated by the camera are not immediately
    suitable for use within the algorithm because of differences in color and
    alignment. This method crops the RGB frame to align with the Depth frame,
    and generates a colorized depth frame useful for debugging.
    '''
    if data['status'] != "READY":
        util.log("Unable to acquire image - system not ready")
        return
    depth = np.asanyarray(data['depth_raw'].get_data())
    color = np.asanyarray(data['color_raw'].get_data())
    data['color_frame'] = cv2.cvtColor(color, cv2.COLOR_RGB2BGR)
    data['depth_frame'] = util.crop(depth, 0.13, 0.14, 0.84, 0.85)
    depth = cv2.applyColorMap(cv2.convertScaleAbs(depth, alpha=0.03), cv2.COLORMAP_JET)
    data['depth_color'] = util.crop(depth, 0.13, 0.14, 0.84, 0.85)

@debug.funcperf
def find_face():
    '''
    Use `dlib` to find faces in the frame. If no face is found, processing will
    stop after this function. If multiple faces are found, only one will be
    used. TODO: Find a way to determine which face will be selected.
    '''
    gray_image = cv2.cvtColor(data['color_frame'], cv2.COLOR_BGR2GRAY)
    scaled_image = util.scale(gray_image, config['face_detection_scale_factor'])
    faces = data['face_detector'](scaled_image)
    if len(faces) == 0:
        raise util.Abort("No faces found in frame", util.Levels.WARN)
    util.log(f"Found {len(faces)} face", util.Levels.DEBUG)
    face = faces[0]
    # Get non-normalized face coordinates (offset scale factor)
    face = type(face)(
        int(face.left()   / config["face_detection_scale_factor"]),
        int(face.top()    / config["face_detection_scale_factor"]),
        int(face.right()  / config["face_detection_scale_factor"]),
        int(face.bottom() / config["face_detection_scale_factor"])
    )
    # Get normalized face coordinates
    data['face_normalized'] = (
        face.left()   / gray_image.shape[1],
        face.top()    / gray_image.shape[0],
        face.right()  / gray_image.shape[1],
        face.bottom() / gray_image.shape[0])
    # Find facial features
    raw_features = data['face_predictor'](image=gray_image, box=face)
    features = np.zeros((68, 3))
    for i in range(0, 68):
        feature = raw_features.part(i)
        x = feature.x / gray_image.shape[1]
        y = feature.y / gray_image.shape[0]
        depth = util.sample(data['depth_frame'], x, y)
        features[i] = [x, y, depth]
    data['named_features'] = {
        "jaw": features[0:17],
        "right_brow": features[17:22],
        "left_brow": features[22:27],
        "nose": features[27:36],
        "right_eye": features[36:42],
        "left_eye": features[42:48],
        "outer_mouth": features[48:60],
        "inner_mouth": features[60:68]
    }

@debug.funcperf
def debug_draw_face(frame='depth_color'):
    '''
    This debug method draws facial features over the image and displays it.
    '''
    if not config['enable_debug_draw']:
        return
    # Create copy of face image
    image = data[frame].copy()
    color = (0, 255, 0)
    thickness = 2
    for i in range(1, len(data['named_features']['jaw'])):
        pt1 = data['named_features']['jaw'][i-1][0:2]
        pt2 = data['named_features']['jaw'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(1, len(data['named_features']['right_brow'])):
        pt1 = data['named_features']['right_brow'][i-1][0:2]
        pt2 = data['named_features']['right_brow'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(1, len(data['named_features']['left_brow'])):
        pt1 = data['named_features']['left_brow'][i-1][0:2]
        pt2 = data['named_features']['left_brow'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(1, len(data['named_features']['nose'])):
        pt1 = data['named_features']['nose'][i-1][0:2]
        pt2 = data['named_features']['nose'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(0, len(data['named_features']['right_eye'])):
        pt1 = data['named_features']['right_eye'][i-1][0:2]
        pt2 = data['named_features']['right_eye'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(0, len(data['named_features']['left_eye'])):
        pt1 = data['named_features']['left_eye'][i-1][0:2]
        pt2 = data['named_features']['left_eye'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(0, len(data['named_features']['outer_mouth'])):
        pt1 = data['named_features']['outer_mouth'][i-1][0:2]
        pt2 = data['named_features']['outer_mouth'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    for i in range(0, len(data['named_features']['inner_mouth'])):
        pt1 = data['named_features']['inner_mouth'][i-1][0:2]
        pt2 = data['named_features']['inner_mouth'][i][0:2]
        util.draw_line(image, pt1, pt2, color, thickness)
    cv2.imshow(f"Debug: {frame}", image)
    cv2.waitKey(delay=1)